---
title: "Stats 506 (F20) Group Project"
author: 'Group 6: Erin Susan Cikanek, Suppapat Korsurat, Kyle William Schulz'
date: "`r format.Date(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    theme: paper
    toc: yes
  pdf_document:
    toc: yes
---

```{r, include = FALSE, echo = FALSE, warning = FALSE}
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(knitr)
```

## GROUP TO DO LIST 
1. Match up language within code between group members how-tos 
2. Summary/Discussion/Reference sections 
3. Match up plotting style as much as possible 
4. Organize git repo
5. Update readme 
6. Improve readme style/info 

### Introduction
Linear regression has become widely known as a backbone of modern statistics. Even as more complex, "black box"-style machine learning techniques increase in popularity, many statisticians and researchers still fall back on regression for its interpretability and simpleness. However, linear regression relies on a number on assumptions that may not always be true in practice, such as the constant, monotonic linearity of predictor variables in relation to the response. In this guide, we explore the use of splines to help model predictor variables that may have changing relationships across their domain. These techniques help us to match the predictive power seen in some more advanced machine learning algorithms while keeping the benefits gained by using regression. We show examples in three popular statistical modelling languages - python, R, and STATA. 

### Data
In this guide, we will be using the "wage" dataset from the R package ISLR. This dataset contains wages from 3,000 Mid-Atlantic workers, along with a select number of other personal demographics. Our goal is to examine the relationship between these demographics and the worker's raw yearly wage. 

### Method
We will first calculate a simple linear regression as a baseline. We will then implement four different spline-like techniques on the "age" predictor variable: a step function, polynomial regression, basis spline, and natural spline. At each step, we will check for fit quality, noting any potential improvements along the way. We will conclude with a retrospective and summary of what we learned.   

### Core Analysis {.tabset}
#### Python
```{r, eval = FALSE}
#!/usr/bin/env python
# coding: utf-8

# In[1]:


#Packages required 
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
#%matplotlib inline
import statsmodels.api as sm
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from patsy import dmatrix


# In[2]:


#Let's read in the data file 
data = pd.read_csv("/Users/kwschulz/STATS506/Stats506_Project/Dataset/data.csv")


# In[3]:


#Take a quick glance at what the data looks like
data.head()


# In[4]:


#Let's check to see if we have any missing values
data.isna().sum()


# In[5]:


#Filter to variables for our analysis 
data = data[["wage", "age", "education", "year"]]


# In[6]:


#Map education to ordinal scale
education_map = {"1. < HS Grad":1,"2. HS Grad":2,
                "3. Some College":3, "4. College Grad":4,
                "5. Advanced Degree":5}
data['education'] = data.education.map(education_map)


# In[7]:


#Lets check the distribution of our predictors
data[["wage", "age"]].hist(layout=(2,1), figsize=(15,15))
plt.show()
plt.savefig('hist.png')


# In[8]:


#checking year distribution
data.year.value_counts().reindex([2003, 2004, 2005, 2006, 2007, 2008, 2009]).plot(kind='bar', 
                                                                                  title='year', 
                                                                                  ylabel='count', 
                                                                                  figsize=(7.5,7.5))
plt.savefig('year_bar.png')


# In[9]:


#checking education distribution 
data.education.value_counts().reindex([1, 2, 3, 4, 5]).plot(kind='bar', 
                                                            title='education', 
                                                            ylabel='count', 
                                                            figsize=(7.5,7.5))
plt.savefig('education_bar.png')


# In[10]:


#linear regression model
model = sm.OLS(data["wage"], sm.add_constant(data.drop('wage',axis=1))).fit()


# In[11]:


#let's check how it did
model.summary()


# In[12]:


#let's cut age into 6 bins - stepwise
data["age_cut"] = pd.cut(data.age, bins=6, labels=False)


# In[13]:


#now let's model age with bins 
model2 = sm.OLS(data["wage"], sm.add_constant(data.drop(['wage','age'],axis=1))).fit()


# In[14]:


#model 2 summary
model2.summary()


# In[15]:


#let's check out the scatter plot of age v wage 
data.plot(x="age", y="wage", kind='scatter', figsize=(7.5,7.5))


# In[16]:


#2nd degree polynomial 
p = np.poly1d(np.polyfit(data["age"], data["wage"], 2))
t = np.linspace(0, 80, 200)
plt.plot(data["age"], data["wage"], 'o', t, p(t), '-')
rs = sm.OLS(data["wage"], 
            np.column_stack([data["age"]**i for i in range(2)]) ).fit().rsquared
plt.title('r2 = {}'.format(rs))
plt.show()
plt.savefig('poly2.png')


# In[17]:


#3rd degree polynomial 
p = np.poly1d(np.polyfit(data["age"], data["wage"], 3))
t = np.linspace(0, 80, 200)
plt.plot(data["age"], data["wage"], 'o', t, p(t), '-')
rs = sm.OLS(data["wage"], 
            np.column_stack([data["age"]**i for i in range(3)]) ).fit().rsquared
plt.title('r2 = {}'.format(rs))
plt.show()
plt.savefig('poly3.png')


# In[18]:


#4th degree polynomial
p = np.poly1d(np.polyfit(data["age"], data["wage"], 4))
t = np.linspace(0, 80, 200)
plt.plot(data["age"], data["wage"], 'o', t, p(t), '-')
rs = sm.OLS(data["wage"], 
            np.column_stack([data["age"]**i for i in range(4)]) ).fit().rsquared
plt.title('r2 = {}'.format(rs))
plt.show()
plt.savefig('poly4.png')


# In[19]:


#5th degree polynomial
p = np.poly1d(np.polyfit(data["age"], data["wage"], 5))
t = np.linspace(0, 80, 200)
plt.plot(data["age"], data["wage"], 'o', t, p(t), '-')
rs = sm.OLS(data["wage"], 
            np.column_stack([data["age"]**i for i in range(5)]) ).fit().rsquared
plt.title('r2 = {}'.format(rs))
plt.show()
plt.savefig('poly5.png')


# In[20]:


#let's do a third polynomial regression 
polynomial_features= PolynomialFeatures(degree=3)
age_p = polynomial_features.fit_transform(data['age'].to_numpy().reshape(-1, 1))
model3 = sm.OLS(data["wage"], sm.add_constant(np.concatenate([data[['education', 'year']].to_numpy(), age_p], axis=1))).fit() 


# In[21]:


#check our results
model3.summary(xname=['education', 'year', 'const', 'poly(age, 3)1', 'poly(age, 3)2', 'poly(age, 3)3'])


# In[22]:


#implementing a bspline for age 
age_bs = dmatrix("bs(data.age, df=6)",{"data.age": data.age}, return_type='dataframe')
model4 = sm.OLS(data["wage"], pd.concat([age_bs, data[['education', 'year']]], axis=1)).fit() 
model4.summary()


# In[23]:


#implementing a natural spline for age 
age_ns = dmatrix("cr(data.age, df=6)",{"data.age": data.age}, return_type='dataframe')
model5 = sm.OLS(data["wage"], pd.concat([age_ns, data[['education', 'year']]], axis=1)).fit() 
model5.summary()


```

```{r, eval = FALSE}
"When you want to show only code, but prevent this chunck to run."
```

```{r, echo = FALSE}
print("When you want this chunck to run, but don't want to show the code.")
```

#### Stata
```{r, eval = FALSE}
"Erin's Code"
```

```{r, eval = FALSE}
"When you want to show only code, but prevent this chunck to run."
```

```{r, echo = FALSE}
print("When you want this chunck to run, but don't want to show the code.")
```

#### R
The library `splines` is required for implementing splines by using R.
```{r}
library(splines)
```

```{r, include = FALSE}
data <- read.csv("/Users/kwschulz/STATS506/Stats506_Project/Dataset/data.csv")
```

```{r scatter_wage_age, include = FALSE}
wage_age <- function(line = FALSE, poly = 1, formula_wage_age = y ~ x){
  plot_title <- "Scatter Plot between Wage and Age"
  
  if(poly != 1){
    plot_title <- paste0("Polynomial degree ", poly)
  }
  
  plot_wage_age <- ggplot(data, aes(x = age, y = wage)) + 
    geom_point(color = "darkblue") +
    theme_bw() +
    labs(title = plot_title, x = "Age", y = "Wage")
  
  if(line == TRUE){
    plot_wage_age + 
      geom_smooth(method = "lm", formula = formula_wage_age, color = "yellow")
  } else {
    plot_wage_age
  }
}
```

First, considering the linear regression.
```{r}
model <- lm(wage ~ age + education + year, data = data)
summary(model)
```

The $R^2$ is 0.2619, which is pretty low. Consider the scatter plot between 
`Wage` and `Age`.

```{r, echo = FALSE, fig.cap = fig_s1}
wage_age()
fig_s1 <- paste0("Figure 3.1 Scatter plot between Wage and Age")
```

The scatter plot show that the relationship between these two variables are not linear. Hence, we will try various types of spline.

##### Step Function
Consider applying the step function on `Age`. 
```{r}
model_cut <- lm(wage ~ cut(age, 4) + education + year, data = data)
summary(model_cut)
```

The $R^2$ is 0.2828, which improved from the previous model. The plot below 
is a scatterplot between `Wage` and `Age`, also the yellow line represents 
the step function.

```{r, echo = FALSE, fig.cap = fig_s2}
wage_age(TRUE, 1, y ~ cut(x,4))
fig_s2 <- paste0("Figure 3.2 Scatter plot between Wage and Age with the
                 step function.")
```

##### Polynomial Regression
Consider the various number for the degree in the polynomial regression. The plots below are the result from the fitting polynomial regression.
```{r, echo = FALSE, warning = FALSE}
degree_poly <- c(2,3,5,20)

for(i in 1:length(degree_poly)){
  poly_d <- degree_poly[i]
  print(wage_age(TRUE, poly_d, y ~ poly(x, poly_d)))
}

```

```{r, echo = FALSE}
poly_r2 <- c()

for(i in 2:20){
  r2_p <-summary(lm(wage ~ poly(age, i) + education + year, 
                   data = data))$r.squared
  poly_r2 <- c(poly_r2, r2_p)
}

poly_r2 <- data.frame(degree = 2:20, R2 = poly_r2)
colnames(poly_r2) <- c("Degree of the age polynomial", "R-Squared")
poly_r2
```

Even the higher degree give the higher $R^2$, the `overfitting` problem may be occured. Hence, polynomial regression with degree 3 would be appropriate.

```{r}
model_poly <- lm(wage ~ poly(age, 3) + education + year, data = data)
summary(model_poly)
```

The $R^2$ is 0.2909, which improved from all previous models.

##### Basis Spline and Natural Spline
For both `Basis Spline` and `Natural Spline`, the number of knots or the degree of freedom need to be specified. One of the method used for specified is performing `K-fold Cross Validation`. In this case, K is equal to 5. For both types of spline, the highest degree of polynomial for age is 3.

  - Basis Spline: df = 4 + knots
  - Natural Spline: df = 2 + knots

```{r k_fold_cv, include = FALSE}
set.seed(1)
data$CV <- sample(rep(1:5, 600))
```

```{r k_fold_plot, include = FALSE}
## Function for calculate and plot
plot_kfold <- function(knots = 1:10, bs = TRUE){
  store_MSE <- c()
  title_plot <- "5-fold cross-validate MSE"
  
  if(bs == TRUE){
    title_plot <- paste0(title_plot, ": Basis Spline")
  } else  {
    title_plot <- paste0(title_plot, ": Natural Spline")
  }
  
  for(i in knots){
    
    MSE <- c()
    
    for(j in 1:5){
      
      ## Split the data
      train_data <- data %>% filter(CV != j)
      validate_data <- data %>% filter(CV == j)
      
      ## Train the model
      if(bs == TRUE){
        model <- lm(wage ~ bs(age, df = 4 + i) + education + year, 
                    data = train_data)
      } else {
        model <- lm(wage ~ ns(age, df = 2 + i) + education + year, 
                    data = train_data)
      }
      
      ## Store the MSE for each validation set
      MSE <- c(MSE, 
               mean((predict(model, validate_data) - validate_data$wage)^2))
      
    }
    
    ## Store the MSE for each knots
    store_MSE <- c(store_MSE, mean(MSE))
  }
  
  ## Create a plot
  dummy_result <- data.frame(knots = as.factor(knots), store_MSE)
  p <- ggplot(dummy_result, aes(x = knots, y = store_MSE, group = 1)) +
    geom_line() +
    geom_point() +
    labs(title = title_plot, x = "Number of knot", y = "MSE") +
    geom_text(aes(label = round(store_MSE,2)), vjust = -1) +
    ylim(min(store_MSE) - 3, max(store_MSE) + 3) +
    theme_bw()
  print(p)
}
```

Consider the MSE for basis spline.

```{r, echo = FALSE}
plot_kfold()
```

The MSE is lowest when the number of `knot` is equal to 2. Fit the regression with basis spline.

```{r}
model_basis <- lm(wage ~ bs(age, df = 6) + education + year, data = data)
summary(model_basis)
```

The $R^2$ is 0.2923.

Then consider the Natural Spline.

```{r, echo = FALSE}
plot_kfold(bs = FALSE)
```

The MSE is lowest when the number of `knot` is equal to 4. Fit the regression with natural spline.

```{r}
model_natural <- lm(wage ~ ns(age, df = 6) + education + year, data = data)
summary(model_natural)
```

The $R^2$ is 0.2927.

### Summary
### Discussion
### Reference


